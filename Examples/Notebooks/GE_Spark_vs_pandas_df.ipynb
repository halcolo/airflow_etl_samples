{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613f3bc8-5522-45dd-ba27-1348b447c462",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Differences between SPARK df vs Pandas df in Great Expectations\n",
    "### Before starting the dependencies we will use\n",
    "```\n",
    "jupyterlab==3.1.13\n",
    "great-expectations==0.13.35\n",
    "pyspark==3.1.2\n",
    "```\n",
    "### Create the df for pandas and based in this the Spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809e1730-cffc-4ee4-90cf-e283a45ce9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juandiegoalfonsoocampo/.pyenv/versions/3.8.5/envs/data_validations/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r9/vvy74cr111j02j295jp2y4b00000gn/T/ipykernel_38450/1822982051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# we can use pandas to avoid needing to define schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m df = spark.createDataFrame(\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mpd_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# first lets create a simple dataframe\n",
    "data = {\n",
    "  \"String\": [\"one\", \"two\", \"two\",],\n",
    "  \"Value\": [1, 2, 2,],\n",
    "}\n",
    "\n",
    "# lets create a pandas dataframe\n",
    "pd_df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# we can use pandas to avoid needing to define schema\n",
    "df = spark.createDataFrame(\n",
    "  pd_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5548282-cc8d-46d2-80e9-71774a8b22fc",
   "metadata": {},
   "source": [
    "### Now let us create the appropriate great-expectations objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d794bd0-37f8-4700-841b-6b69b383fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pandas we create a great expectations object like this\n",
    "pd_df_ge = ge.from_pandas(pd_df) \n",
    "\n",
    "# while for pyspark we can do it like this\n",
    "df_ge = ge.dataset.SparkDFDataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51389dc-5d2b-4fc2-8a5d-4e40dcd00295",
   "metadata": {},
   "source": [
    "# Running Great Expectations tests\n",
    "\n",
    "Expectations return a dictionary of metadata, including a boolean \"success\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdb940-2906-4dd7-a391-2e7acfc82b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this works the same for bot Panmdas and PySpark Great Expectations datasets\n",
    "print(pd_df_ge.expect_table_row_count_to_be_between(1,10))\n",
    "\n",
    "print(df_ge.expect_table_row_count_to_be_between(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997db5f4-7a6d-429e-b65c-82648caffd51",
   "metadata": {},
   "source": [
    "# Differences between Great Expectations Pandas and Pyspark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75e39d-7ed6-4222-a9f1-df7756c43fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas datasets inherit all the pandas dataframe methods\n",
    "print(pd_df_ge.count())\n",
    "\n",
    "# while GE pyspark datasets do not and the following leads to an error\n",
    "print(df_ge.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f474fa6-f03e-4871-a3ee-bd6914a01cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# however you can access the original pyspark dataframe using df_ge.spark_df\n",
    "df_ge.spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91900627-d197-467f-84af-fecb4502f219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
